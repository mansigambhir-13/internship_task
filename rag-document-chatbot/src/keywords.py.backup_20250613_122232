"""
Simple Keywords Extraction Script using Groq Llama
Processes chunked documents and extracts contextual keywords
Simplified version with minimal dependencies
"""

import json
import os
import time
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
import logging
import requests

# Load .env file if it exists
def load_env_file():
    """Load environment variables from .env file"""
    # Try using python-dotenv first
    try:
        from dotenv import load_dotenv
        load_dotenv()
        print("‚úÖ Loaded .env file using python-dotenv")
        return
    except ImportError:
        pass
    
    # Fallback to manual loading
    env_file = Path('.env')
    if env_file.exists():
        print("üìÑ Loading .env file manually...")
        with open(env_file, 'r') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and '=' in line:
                    key, value = line.split('=', 1)
                    # Clean the value (remove quotes)
                    value = value.strip().strip('"').strip("'")
                    os.environ[key.strip()] = value
                    print(f"   Loaded: {key.strip()}")
        print("‚úÖ Loaded .env file manually")
    else:
        print("‚ö†Ô∏è .env file not found in current directory")
        print(f"   Current directory: {Path.cwd()}")
        print("   Looking for .env file with GROQ_API_KEY=your_key")

# Load environment variables
load_env_file()

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class SimpleKeywordExtractor:
    def __init__(self, project_root: str = None):
        """Initialize the keyword extractor"""
        
        # Set up paths
        if project_root:
            self.project_root = Path(project_root)
        else:
            self.project_root = Path(__file__).parent.parent
        
        self.input_dir = self.project_root / "data" / "chunked_documents"
        self.output_dir = self.project_root / "data" / "keywords_enhanced"
        
        # Create output directory
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Groq API configuration
        self.api_key = os.getenv('GROQ_API_KEY', '')
        self.base_url = "https://api.groq.com/openai/v1/chat/completions"
        self.model = "llama3-8b-8192"
        
        # Rate limiting - Adjusted for Groq free tier
        self.rate_limit_delay = 2.5  # 2.5 seconds between requests (safe for free tier)
        
        logger.info(f"SimpleKeywordExtractor initialized")
        logger.info(f"Project root: {self.project_root}")
        logger.info(f"Input directory: {self.input_dir}")
        logger.info(f"Output directory: {self.output_dir}")
        logger.info(f"Groq API key configured: {'Yes' if self.api_key else 'No'}")
    
    def check_setup(self) -> bool:
        """Check if everything is set up correctly"""
        
        # Check API key
        if not self.api_key:
            print("‚ùå Groq API key not found")
            print("üí° Please set GROQ_API_KEY environment variable")
            print("   Get your free API key from: https://console.groq.com/")
            return False
        
        # Check input directory
        if not self.input_dir.exists():
            print(f"‚ùå Input directory not found: {self.input_dir}")
            print("üí° Please run chunking.py first")
            return False
        
        # Check consolidated chunks file
        consolidated_file = self.input_dir / "all_chunks.json"
        if not consolidated_file.exists():
            print(f"‚ùå Consolidated chunks file not found: {consolidated_file}")
            print("üí° Please run chunking.py first to create all_chunks.json")
            return False
        
        return True
    
    def test_api_connection(self) -> bool:
        """Test if Groq API is working"""
        try:
            test_keywords = self.extract_keywords_from_text("Test document about financial markets.")
            if test_keywords:
                print("‚úÖ Groq API connection successful")
                return True
            else:
                print("‚ùå Groq API test failed")
                return False
        except Exception as e:
            print(f"‚ùå Groq API connection failed: {e}")
            return False
    
    def create_prompt(self, text: str) -> str:
        """Create prompt for keyword extraction"""
        
        prompt = f"""Extract exactly 5 relevant keywords from this financial/business text. Focus on:
- Key companies, organizations, people
- Financial terms and amounts  
- Regulatory or legal terms
- Industry-specific terminology
- Important concepts

TEXT: {text}

Return ONLY a JSON array of 5 keywords:
["keyword1", "keyword2", "keyword3", "keyword4", "keyword5"]"""

        return prompt
    
    def extract_keywords_from_text(self, text: str) -> Optional[List[str]]:
        """Extract keywords using Groq API"""
        
        if not text or len(text.strip()) < 20:
            return []
        
        try:
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            payload = {
                "model": self.model,
                "messages": [
                    {
                        "role": "user", 
                        "content": self.create_prompt(text)
                    }
                ],
                "temperature": 0.3,
                "max_tokens": 150,
                "top_p": 0.9
            }
            
            response = requests.post(
                self.base_url,
                headers=headers,
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                content = result['choices'][0]['message']['content'].strip()
                
                # Parse JSON response
                try:
                    # Find JSON array in response
                    if '[' in content and ']' in content:
                        start = content.find('[')
                        end = content.find(']') + 1
                        json_str = content[start:end]
                        keywords = json.loads(json_str)
                        
                        # Clean and validate keywords
                        cleaned_keywords = []
                        for kw in keywords:
                            if isinstance(kw, str) and len(kw.strip()) > 0:
                                cleaned_keywords.append(kw.strip().lower())
                        
                        return cleaned_keywords[:5]
                
                except json.JSONDecodeError:
                    # Fallback: extract keywords manually
                    return self.extract_keywords_manually(content)
            
            elif response.status_code == 429:
                # Rate limit hit - exponential backoff
                wait_time = 10  # Start with 10 seconds
                print(f"‚è≥ Rate limit hit, waiting {wait_time} seconds...")
                time.sleep(wait_time)
                # Try again with longer delay
                self.rate_limit_delay = min(self.rate_limit_delay * 1.2, 5.0)  # Increase delay
                return self.extract_keywords_from_text(text)
            
            else:
                logger.error(f"API error: {response.status_code} - {response.text}")
                return None
                
        except Exception as e:
            logger.error(f"Error extracting keywords: {e}")
            return None
    
    def extract_keywords_manually(self, response_text: str) -> List[str]:
        """Manual keyword extraction from malformed response"""
        import re
        
        keywords = []
        
        # Try to find quoted words
        quoted_words = re.findall(r'"([^"]*)"', response_text)
        for word in quoted_words:
            if len(word.strip()) > 2:
                keywords.append(word.strip().lower())
        
        # If no quoted words, try comma-separated
        if not keywords:
            # Clean response and split
            clean_text = re.sub(r'^.*?:', '', response_text)  # Remove prefix
            potential_keywords = re.split(r'[,\n\-\*\d+\.\s]+', clean_text)
            
            for kw in potential_keywords:
                cleaned = kw.strip().lower()
                if len(cleaned) > 2 and not cleaned.isdigit():
                    keywords.append(cleaned)
        
        return keywords[:5]
    
    def process_chunks(self, chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Process all chunks and add keywords"""
        
        enhanced_chunks = []
        total_chunks = len(chunks)
        
        print(f"üîÑ Processing {total_chunks} chunks...")
        
        for i, chunk in enumerate(chunks, 1):
            try:
                text = chunk.get('text', '')
                
                if not text:
                    enhanced_chunks.append(chunk)
                    continue
                
                # Show progress
                if i % 10 == 0 or i == total_chunks:
                    print(f"üìä Progress: {i}/{total_chunks} ({i/total_chunks*100:.1f}%)")
                
                # Extract keywords
                keywords = self.extract_keywords_from_text(text)
                
                # Update chunk - ensure consistent field names
                enhanced_chunk = {
                    "doc_id": chunk.get('doc_id'),
                    "page": chunk.get('page'),
                    "paragraph_number": chunk.get('paragraph_number', chunk.get('para_id')),  # Handle both field names
                    "text": chunk.get('text'),
                    "keywords": keywords if keywords else []
                }
                
                enhanced_chunks.append(enhanced_chunk)
                
                # Rate limiting
                if i < total_chunks:
                    time.sleep(self.rate_limit_delay)
                
            except Exception as e:
                logger.error(f"Error processing chunk {i}: {e}")
                # Keep original structure but ensure field consistency
                fallback_chunk = {
                    "doc_id": chunk.get('doc_id'),
                    "page": chunk.get('page'),
                    "paragraph_number": chunk.get('paragraph_number', chunk.get('para_id')),
                    "text": chunk.get('text'),
                    "keywords": []
                }
                enhanced_chunks.append(fallback_chunk)
        
        return enhanced_chunks
    
    def run_extraction(self) -> bool:
        """Main extraction process"""
        
        # Load consolidated chunks
        consolidated_file = self.input_dir / "all_chunks.json"
        
        try:
            print(f"üìÇ Loading chunks from: {consolidated_file}")
            
            with open(consolidated_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            chunks = data.get('chunks', [])
            total_chunks = len(chunks)
            
            if total_chunks == 0:
                print("‚ùå No chunks found")
                return False
            
            print(f"üìä Found {total_chunks} chunks to process")
            
            # Estimate time
            estimated_time = total_chunks * self.rate_limit_delay / 60
            print(f"‚è±Ô∏è  Estimated processing time: {estimated_time:.1f} minutes")
            
            # Confirm
            response = input(f"\nüöÄ Start processing? (yes/no): ")
            if response.lower() != 'yes':
                print("‚ùå Processing cancelled")
                return False
            
            # Process chunks
            start_time = time.time()
            enhanced_chunks = self.process_chunks(chunks)
            end_time = time.time()
            
            # Update data structure
            enhanced_data = data.copy()
            enhanced_data['chunks'] = enhanced_chunks
            enhanced_data['metadata']['keywords_extracted'] = True
            enhanced_data['metadata']['extraction_completed_at'] = datetime.now().isoformat()
            enhanced_data['metadata']['groq_model'] = self.model
            
            # Save results
            output_file = self.output_dir / "all_chunks_with_keywords.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(enhanced_data, f, indent=2, ensure_ascii=False)
            
            # Calculate statistics
            chunks_with_keywords = sum(1 for chunk in enhanced_chunks if chunk.get('keywords'))
            processing_time = (end_time - start_time) / 60
            
            print(f"\n‚úÖ KEYWORD EXTRACTION COMPLETED!")
            print(f"‚è±Ô∏è  Processing time: {processing_time:.1f} minutes")
            print(f"üìä Results:")
            print(f"   Total chunks: {total_chunks}")
            print(f"   Chunks with keywords: {chunks_with_keywords}")
            print(f"   Success rate: {chunks_with_keywords/total_chunks*100:.1f}%")
            print(f"üìÅ Output saved to: {output_file}")
            
            # Show sample results
            print(f"\nüîë SAMPLE KEYWORDS:")
            for i, chunk in enumerate(enhanced_chunks[:3]):
                keywords = chunk.get('keywords', [])
                if keywords:
                    text_preview = chunk.get('text', '')[:80] + "..."
                    print(f"   {i+1}. Keywords: {keywords}")
                    print(f"      Text: {text_preview}\n")
            
            # Save simple statistics
            stats = {
                "processing_summary": {
                    "total_chunks": total_chunks,
                    "chunks_with_keywords": chunks_with_keywords,
                    "success_rate": round(chunks_with_keywords/total_chunks*100, 2),
                    "processing_time_minutes": round(processing_time, 2)
                },
                "top_keywords": self.get_top_keywords(enhanced_chunks),
                "processed_at": datetime.now().isoformat()
            }
            
            stats_file = self.output_dir / "extraction_summary.json"
            with open(stats_file, 'w', encoding='utf-8') as f:
                json.dump(stats, f, indent=2, ensure_ascii=False)
            
            print(f"üìà Statistics saved to: {stats_file}")
            
            return True
            
        except Exception as e:
            print(f"‚ùå Error during extraction: {e}")
            return False
    
    def get_top_keywords(self, chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Get top keywords by frequency"""
        
        keyword_counts = {}
        
        for chunk in chunks:
            keywords = chunk.get('keywords', [])
            for keyword in keywords:
                keyword_counts[keyword] = keyword_counts.get(keyword, 0) + 1
        
        # Sort by frequency
        top_keywords = sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)[:15]
        
        return [{"keyword": kw, "count": count} for kw, count in top_keywords]

def main():
    """Main function"""
    
    print("üîë KEYWORD EXTRACTION WITH GROQ LLAMA")
    print("=" * 60)
    
    # Initialize extractor
    try:
        extractor = SimpleKeywordExtractor()
    except Exception as e:
        print(f"‚ùå Failed to initialize extractor: {e}")
        return
    
    # Check setup
    if not extractor.check_setup():
        return
    
    # Test API
    if not extractor.test_api_connection():
        return
    
    # Run extraction
    success = extractor.run_extraction()
    
    if success:
        print("\nüéâ All done! Your chunks now have keywords for better RAG retrieval.")
    else:
        print("\n‚ùå Extraction failed. Check the logs above for details.")

if __name__ == "__main__":
    main()